{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuryaMurali436/suryamurali436.github.io/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee991a7",
      "metadata": {
        "id": "1ee991a7"
      },
      "source": [
        "## Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "652b25b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "652b25b7",
        "outputId": "cd7dfce3-a465-49bd-d379-c7759e871b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c12d5e1",
      "metadata": {
        "id": "8c12d5e1"
      },
      "source": [
        "## Importing the URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0f46dd82",
      "metadata": {
        "id": "0f46dd82"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.bbc.com/news/world\"\n",
        "html = urllib.request.urlopen(url).read()\n",
        "soup = BeautifulSoup(html, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "56fc73ba",
      "metadata": {
        "id": "56fc73ba"
      },
      "outputs": [],
      "source": [
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d26a2b",
      "metadata": {
        "id": "b2d26a2b"
      },
      "source": [
        "## Getting the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ba64e588",
      "metadata": {
        "id": "ba64e588"
      },
      "outputs": [],
      "source": [
        "text = soup.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "44fed4a0",
      "metadata": {
        "id": "44fed4a0"
      },
      "outputs": [],
      "source": [
        "lines = (line.strip() for line in text.splitlines())\n",
        "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "text = ' '.join(chunk for chunk in chunks if chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179ea99d",
      "metadata": {
        "id": "179ea99d"
      },
      "source": [
        "## Cheking the word frequency "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30baae3c",
      "metadata": {
        "id": "30baae3c"
      },
      "outputs": [],
      "source": [
        "word_freq = {}\n",
        "for word in word_tokenize(text):\n",
        "    if word not in word_freq.keys():\n",
        "        word_freq[word] = 1\n",
        "    else:\n",
        "        word_freq[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "69c72913",
      "metadata": {
        "id": "69c72913"
      },
      "outputs": [],
      "source": [
        "words_dict = {}\n",
        "ps = PorterStemmer()\n",
        "for word in word_freq.keys():\n",
        "    words_dict[ps.stem(word)] = word_freq[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7a3f7277",
      "metadata": {
        "id": "7a3f7277"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4f024062",
      "metadata": {
        "id": "4f024062"
      },
      "outputs": [],
      "source": [
        "words_ignore = []\n",
        "for word in words_dict.keys():\n",
        "    if (word in stop_words) or (len(word) == 1):\n",
        "        words_ignore.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d1246196",
      "metadata": {
        "id": "d1246196"
      },
      "outputs": [],
      "source": [
        "words_use = []\n",
        "for word in words_dict.keys():\n",
        "    if word not in words_ignore:\n",
        "        words_use.append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b54872e",
      "metadata": {
        "id": "5b54872e"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88259f4",
      "metadata": {
        "id": "f88259f4"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 200)\n",
        "X = vectorizer.fit_transform(words_use)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae8b1f7",
      "metadata": {
        "id": "5ae8b1f7"
      },
      "source": [
        "## K means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34de2c21",
      "metadata": {
        "id": "34de2c21"
      },
      "outputs": [],
      "source": [
        "true_k = 5\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eed6b40",
      "metadata": {
        "id": "4eed6b40"
      },
      "outputs": [],
      "source": [
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "for i in range(true_k):\n",
        "    print(\"Cluster %d:\" % i)\n",
        "for ind in order_centroids[i, :10]:\n",
        "    print(' %s' % terms[ind])\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}