{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuryaMurali436/suryamurali436.github.io/blob/main/_Untitled13_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWGZR2X3mo7l"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.rdd import RDD\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import functions\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import lit, desc, col, size, array_contains\\\n",
        ", isnan, udf, hour, array_min, array_max, countDistinct\n",
        "from pyspark.sql.types import *\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import isnan, when, count, col, translate\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark import SparkConf, SparkContext, SQLContext\n",
        "conf = SparkConf().setAppName(\"test\").setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "metadata": {
        "id": "sEFM2pdInqF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sc)\n",
        "print(sqlContext)"
      ],
      "metadata": {
        "id": "XdU92zannsgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"how to read csv file\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "QFBU4cnBn9Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.csv(\"city_day.csv\",inferSchema=True,header=True)"
      ],
      "metadata": {
        "id": "CMn1dfjUoDCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.show()"
      ],
      "metadata": {
        "id": "PU5mSalUsfcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.read.csv(\"city_hour.csv\",inferSchema=True,header=True)"
      ],
      "metadata": {
        "id": "tKLE-E913yMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "id": "3zNHu8wZ36_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = spark.read.csv(\"stations.csv\",inferSchema=True,header=True)"
      ],
      "metadata": {
        "id": "-rPkwBQM3-Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.show()"
      ],
      "metadata": {
        "id": "BWzJxMfT4RBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = spark.read.csv(\"station_hour.csv\",inferSchema=True,header=True)"
      ],
      "metadata": {
        "id": "cciheLbd4T-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.show()"
      ],
      "metadata": {
        "id": "w5qMZmKL4arR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = spark.read.csv(\"station_day.csv\",inferSchema=True,header=True)"
      ],
      "metadata": {
        "id": "1rfTSsqC4avM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.show()"
      ],
      "metadata": {
        "id": "wOoLRQOD4azc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "!pip3 install -q findspark\n"
      ],
      "metadata": {
        "id": "hB6rW2o-5kCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "J2k2mJrN5cdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "if3O8LvR5cw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('Group_74').getOrCreate()"
      ],
      "metadata": {
        "id": "DGodJ9DK5c0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMu5HVRi5-ud",
        "outputId": "fa443326-04cc-46a1-850d-3c9c2b008934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce # For Python 3.x\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def unionAll(*dfs):\n",
        "\treturn reduce(DataFrame.unionAll, dfs)\n",
        "\n",
        "IDS_df= unionAll(df1,df2,df4,df5)"
      ],
      "metadata": {
        "id": "kjHjBaYG4a59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "import functools\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "p1gKKzcf4ccX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDS_df.show()"
      ],
      "metadata": {
        "id": "kFN67fo17LiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDS_df.select('City').groupBy('City').count().orderBy('count', ascending=False).show()"
      ],
      "metadata": {
        "id": "crpP0hhy7NXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDS_df.show()"
      ],
      "metadata": {
        "id": "L6H-CJQy7bwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDS_df1= IDS_df.dropna()"
      ],
      "metadata": {
        "id": "IiMd85dL7lUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDS_df1.show()"
      ],
      "metadata": {
        "id": "4nCHasle7q0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attackdf=IDS_df1"
      ],
      "metadata": {
        "id": "4p6hOe5d7tgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "attackdf=IDS_df1.select(col(\"City\").cast('float').alias('City'), \n",
        "                             col(\"Date\").cast('float').alias('Date'),\n",
        "                             col(\"PM2\").cast('float').alias('PM2'),\n",
        "                             col(\"NO\").cast('float').alias('NO'),\n",
        "                             col(\"AQI_Bucket\").cast('float').alias('AQI_Bucket'),\n",
        "                             col(\"NO2\").cast('float').alias('NO2'),\n",
        "                             col(\"NOx\").cast('float').alias('NOx'),\n",
        "                             col(\"NH3\").cast('float').alias('NH3'),\n",
        "                             col(\"CO\").cast('float').alias('CO'),\n",
        "                             col(\"SO2\").cast('float').alias('SO2'),\n",
        "                             col(\"O3\").cast('float').alias('O3'),\n",
        "                             col(\"Benzene\").cast('float').alias('Benzene'),\n",
        "                             col(\"Toluene\").cast('float').alias('Toluene'),\n",
        "                             col(\"Xylene\").cast('float').alias('Xylene'),\n",
        "                             col(\"AQI\").cast('float').alias('AQI'))"
      ],
      "metadata": {
        "id": "ukwpsEXi7w7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attackdf.na.fill(value=0,subset=[\"AQI\"]).show()"
      ],
      "metadata": {
        "id": "UhCs_0n-9K8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attackdf.printSchema()"
      ],
      "metadata": {
        "id": "S_vHg-B-9aPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"AQI\", outputCol=\"AQIindex\")\n",
        "index_1 = indexer.fit(attackdf).transform(attackdf)"
      ],
      "metadata": {
        "id": "bUtrnY3W9hpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_1.show()"
      ],
      "metadata": {
        "id": "2i1OcG9O9uXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_1.select(\"AQIindex\").distinct().show()"
      ],
      "metadata": {
        "id": "iKEy46Yi9z0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_1.show()"
      ],
      "metadata": {
        "id": "ZVdQ0J0j96Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.stat import ChiSquareTest"
      ],
      "metadata": {
        "id": "OeYXeswpANvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_1.approxQuantile(\"AQIindex\", [0.5], 0.25)\n"
      ],
      "metadata": {
        "id": "dDSZj89kAlSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
        "\n",
        "index_stat = index_1.select((_stddev(col('AQIindex')))*(_stddev(col('AQIindex'))).alias('AQI')).collect()\n",
        "index_stat"
      ],
      "metadata": {
        "id": "er_ENvtzAqJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_stat = index_1.select( _stddev(col('AQI')).alias('std')).collect()"
      ],
      "metadata": {
        "id": "qBmzSfBeA6lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_stat"
      ],
      "metadata": {
        "id": "04ZM0FOJBGmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_1.describe().show()"
      ],
      "metadata": {
        "id": "_LHlpU-pBMzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_stat = index_1.select(\n",
        "    _mean(col('AQI')).alias('mean')\n",
        ").collect()\n",
        "\n",
        "mean = index_stat[0]['mean']\n",
        "print(mean)\n"
      ],
      "metadata": {
        "id": "V9ZVmgCPBR5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark-dist-explore"
      ],
      "metadata": {
        "id": "KSgB3Tf7Bbt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark_dist_explore import hist\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "hist(ax, index_1.select('AQI'), bins = 20, color=['black'])"
      ],
      "metadata": {
        "id": "kx-MpZDFBpcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_stat = index_1.select(\n",
        "    _stddev(col('AQI')).alias('std')\n",
        ").collect()\n",
        "\n",
        "std = index_stat[0]['std']\n",
        "print(std)"
      ],
      "metadata": {
        "id": "nQX-JFQ2Bxes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.getOrCreate()\n",
        "from pyspark.ml.stat import Correlation\n",
        "index_1.corr(\"AQI\",\"Benzene\" )"
      ],
      "metadata": {
        "id": "VzSLxW7ADbfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature  import VectorAssembler"
      ],
      "metadata": {
        "id": "4z0B6dm8B7nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorcol1=\"corr-features\""
      ],
      "metadata": {
        "id": "dgUyjCqLCciL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_1.columns"
      ],
      "metadata": {
        "id": "wJRn5Qe6Ce3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler=VectorAssembler(inputCols=['AQI','AQIindex','SO2'], outputCol=vectorcol1)"
      ],
      "metadata": {
        "id": "UzQ3NEngCiX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector1=assembler.transform(index_1).select(vectorcol1)"
      ],
      "metadata": {
        "id": "h6inbcc7C8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector1.show()"
      ],
      "metadata": {
        "id": "Cb7v1vg9DAYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mat=Correlation.corr(vector1,vectorcol1).collect()[0][0]"
      ],
      "metadata": {
        "id": "msOSZZEXDGTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrmatrix=mat.toArray().tolist()\n",
        "print(mat)"
      ],
      "metadata": {
        "id": "zcDSusQIDKCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession.builder.appName(\"lin_reg\").getOrCreate()\n",
        "data=index_1.na.drop()\n"
      ],
      "metadata": {
        "id": "CAo48fqCD3aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "id": "3QGcMJguD-eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "J0F2slsVEES5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data=index_1"
      ],
      "metadata": {
        "id": "6g0B2se8ELNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler= VectorAssembler(inputCols=['O3','AQI'],\n",
        "                           outputCol='features')"
      ],
      "metadata": {
        "id": "lYEfjr5fEO9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data1=assembler.transform(new_data)\n"
      ],
      "metadata": {
        "id": "FqSBIZ8tEfQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data1.show()"
      ],
      "metadata": {
        "id": "8p9xCK1oEibR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data1 = new_data1.select(['features', 'AQIindex',])"
      ],
      "metadata": {
        "id": "sgQQBtlxElIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe=new_data1.select(col(\"AQIindex\").cast('integer').alias('AQIindex'), col('features'))"
      ],
      "metadata": {
        "id": "XPAeU3leEwAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.show()"
      ],
      "metadata": {
        "id": "MT4chpacE8wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = dataframe.randomSplit([0.7, 0.3], seed = 2018)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))"
      ],
      "metadata": {
        "id": "oLc7li_rFAO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol='AQIindex', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "lr_model = lr.fit(train)\n",
        "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
        "print(\"Intercept: \" + str(lr_model.intercept))"
      ],
      "metadata": {
        "id": "eGijaJJsFD0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainingSummary = lr_model.summary\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"r2: %f\" % trainingSummary.r2)"
      ],
      "metadata": {
        "id": "MDaV-wylFMO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gblHd4mv1Qxv"
      },
      "outputs": [],
      "source": [
        "#\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Classification ').getOrCreate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'LabelIndex', maxIter=10)\n",
        "lrModel = lr.fit(train)"
      ],
      "metadata": {
        "id": "goPZv2USjnzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lrModel.transform(test)\n",
        "predictions.printSchema()"
      ],
      "metadata": {
        "id": "UPDMuKpajqvZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}